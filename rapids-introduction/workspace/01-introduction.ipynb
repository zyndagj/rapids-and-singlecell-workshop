{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "160bf2b9-a4af-45f7-aa02-dd54a2fb58eb",
   "metadata": {},
   "source": [
    "# GPU-Accelerated Data Science in Python\n",
    "\n",
    "Workshop exploring methods for GPU-accelerated data science in the Python programming language.\n",
    "\n",
    "Requirements:\n",
    "- NVIDIA GPU\n",
    "- Container runtime for [nvcr.io/nvidia/pytorch:25.03-py3](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch) or [nvcr.io/nvidia/rapidsai/base:25.04-cuda12.8-py3.11](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/rapidsai/containers/base/tags)\n",
    "    - with [jupyterlab_nvdashboard](https://github.com/rapidsai/jupyterlab-nvdashboard)\n",
    "- Experience with Python\n",
    " \n",
    "Outcomes:\n",
    "- Explore GPU-accelerated Data Science Packages\n",
    "- Understand when GPU acceleration makes sense\n",
    "- Learn how to work with data that doesn't fit in GPU memory\n",
    "- Learn how to across multiple GPUs\n",
    "\n",
    "## Introduction\n",
    "<hr>\n",
    "\n",
    "GPU-accelerated data science involves using GPUs to perform data-intensive computations more quickly than traditional Central Processing Units (CPUs).\n",
    "GPUs excel in tasks that involve large-scale parallel processing, making them ideal for machine learning, deep learning, data processing, and scientific computing.\n",
    "\n",
    "This workshop is meant to be generally applicable who already use python for data science.\n",
    "We'll be learning how to adapt CPU-based data-science methods to run on the GPU.\n",
    "\n",
    "## Quickstart with K-means on the GPU\n",
    "<hr>\n",
    "\n",
    "If you only have a few minutes, this section will get you started with GPU-accelerated data science.\n",
    "We'll be using the k-means algorithm as our example algorithm because the methods are relatively simple and it has been re-implemented in many different python packages.\n",
    "\n",
    "If you've never use the k-means algorith before, its goal is to partition data into `k` clusters by minimizing the variance within each cluster.\n",
    "The algorithm starts by:\n",
    "\n",
    "1. Initializing `k` centroids randomly (usually by choosing random data points without replacement)\n",
    "1. Each data point is then assigned to the nearest centroid\n",
    "1. The centroids are updated to be the mean of the assigned points\n",
    "1. This process is repeated until convergence, where assignments no longer change.\n",
    "\n",
    "If you're interested in learning more, check out the [wikipedia page](https://en.wikipedia.org/wiki/K-means_clustering).\n",
    "\n",
    "### Starting from scratch\n",
    "\n",
    "While most folks usually start from a pre-implemented version of k-means, lets say you wrote a python script like [kmeans.py](kmeans.py). This script implements the k-means algorithm using array based operations in [numpy](https://numpy.org/).\n",
    "\n",
    "Take a look at the script to see what it's doing, but no need to master the content.\n",
    "\n",
    "You'll notice that it focuses on minimizing `for` loops by using numpy's array-based operations.\n",
    "This is necessary for performance in python since loops cannot be vectorized.\n",
    "\n",
    "Since the script is in the same directory as this notebook, we can import it and run it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6194d643-e53b-43bb-85da-0bd12b687634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS']=str(min(16,os.cpu_count())) # Necessary on nodes with many cores\n",
    "\n",
    "from kmeans import k_means # Load our numpy implementation of k-means\n",
    "import numpy as np # Use numpy to create and work with data\n",
    "import matplotlib.pyplot as plt # We'll plot the data\n",
    "from time import time # Allows us to time blocks of code\n",
    "\n",
    "# Create some data\n",
    "data = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [99, 100]])\n",
    "k = 3  # Number of clusters\n",
    "\n",
    "# Run K-means\n",
    "assignments, centroids = k_means(data, k)\n",
    "\n",
    "# Print the results (you can modify this for your needs)\n",
    "print(\"Final Centroids:\\n\", centroids)\n",
    "print(\"Cluster Assignments:\", assignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189bf646-853d-4091-9ae8-3155cd35b24c",
   "metadata": {},
   "source": [
    "### Generating large data\n",
    "\n",
    "This numpy implementation works great for our current data.\n",
    "What happens if data grows though?\n",
    "\n",
    "To answer this question, we can create a function to generate large quantities of clustered data to see how this runs as data scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95b7f11-6498-4e2c-9203-8ba5c516fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(k=3, n=10, d=2, cmin=0, cmax=10):\n",
    "    '''\n",
    "    Generate a random dataset for clustering.\n",
    "    k - number of clusters\n",
    "    n - number of data points\n",
    "    d - dimension of data points\n",
    "    cmin - smallest centroid value\n",
    "    cmax - largest centroid value\n",
    "    '''\n",
    "    # Generate random integers for centroids\n",
    "    C = np.random.randint(cmin, cmax, size=(k,d))\n",
    "    # Create data points by adding random noise to centroids\n",
    "    D = np.random.randn(n, d)+C[np.random.choice(k, n, replace=True)]\n",
    "    return D, C\n",
    "\n",
    "data, centroids = generate_data(k=3, n=15, d=2)\n",
    "print(\"Centroids:\\n\",centroids,\"\\n\")\n",
    "print(\"Data Points:\\n\",data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c923a3-50bb-4d0f-88ee-834ca55f3c61",
   "metadata": {},
   "source": [
    "### Plotting 2D clusters\n",
    "\n",
    "To help understand the data that is generated, we can visualize it.\n",
    "So we don't have to cover how to use [Matplotlib](https://matplotlib.org/) in this workshop, the [extras.py](extras.py) script contains a `plot_clusters` function for plotting 2D data.\n",
    "\n",
    "Feel free to regernate data if the clusters don't make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f59c517-0be7-49ac-bbc1-1248b84eb64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extras import plot_clusters\n",
    "\n",
    "plot_clusters(data, actual=centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40bc800-381a-49ba-8c1d-d2db6ec25e68",
   "metadata": {},
   "source": [
    "### Running K-means\n",
    "\n",
    "Try running the numpy implementation of K-means using the `k_means` function and visualize the cluster assignments using `plot_clusters` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b0a771-01ba-4758-b1c0-c8fbd5571cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments, calc_centroids = k_means(data, k=3)\n",
    "print(calc_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4077681-2259-48fd-a124-0c3e1e9fb5bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_clusters(data, assignments, k=3, calculated=calc_centroids, actual=centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad667b99-786f-4073-a417-363a37e6826d",
   "metadata": {},
   "source": [
    "This is mostly to prove we can now generate clusters of data in any shape.\n",
    "I'm focusing on the centroids here so we don't have to get into assignment accuracy.\n",
    "As the number of data points increases, the calculated centroids should get closer to actual centroids used to generate the data.\n",
    "Feel free to take some time to try increasing `n` when `generate_data` is called to verify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1f0b4e-be23-4afe-b1ed-7e1c2550f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different counts and dimensions of data\n",
    "\n",
    "data, centroids = generate_data(k=3, n=100, d=2)\n",
    "plot_clusters(data, actual=centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fc3685-1fe0-4092-a0f3-2f4dac3c698c",
   "metadata": {},
   "source": [
    "### Timing numpy implementation\n",
    "\n",
    "Now, lets look at runtime as the data grows from 10 to 10,000,000 (10^7) data points.\n",
    "\n",
    "Notice that we're only running K-means for a single iteration with `max_iterations=1`.\n",
    "This is because the k-means algorithm is an NP-hard algorithm with a non-deterministic runtime (all the random selections) because it loops until convergence.\n",
    "To give us an idea of how fast each loop of the algorithm runs, we're just doing a single iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a9b8e3-0c15-4803-aae6-f099ca14b614",
   "metadata": {},
   "outputs": [],
   "source": [
    "k, d = 4, 10\n",
    "\n",
    "for N in [10**power for power in range(1,8)]:\n",
    "    data, real_centroids = generate_data(k=k, n=N, d=d, cmin=0, cmax=100)\n",
    "    start_time = time()\n",
    "    # Time a single iteration because k-means is non-deterministic\n",
    "    k_means(data, k, max_iterations=1)\n",
    "    elapsed_time = time()-start_time\n",
    "    print(f\"Finished kmeans on {N} points in {np.round(elapsed_time,4)} seconds\")\n",
    "    del data, real_centroids\n",
    "print(\"Done\") # I sometimes wait for another step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb933c8-0f7a-4b86-87f0-9831a372df2f",
   "metadata": {},
   "source": [
    "### Timing Scikit-learn implementation\n",
    "\n",
    "You may already know this, but the k-means algorithm and may others already exist in a package called [scikit-learn](https://scikit-learn.org/stable/), an open-source, widely-used Python package for machine learning. It provides a comprehensive set of algorithms for classification, regression, clustering, dimensionality reduction, and more, along with tools for model selection, data preprocessing, and visualization.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/scikit-learn/scikit-learn/refs/heads/main/doc/logos/scikit-learn-logo.png\" alt=\"scikit\" style=\"margin: 0 auto;\" />\n",
    "\n",
    "The `KMeans` class exists in the \"cluster\" collection.\n",
    "Take a look at the documentation to learn how to use it.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac469257-cfdc-4c3d-94ce-0d8d2185f021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS']=str(min(16,os.cpu_count())) # Necessary on nodes with many cores\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k, d = 4, 10\n",
    "\n",
    "for N in [10**power for power in range(1,8)]:\n",
    "    data, real_centroids = generate_data(k=k, n=N, d=d, cmin=0, cmax=100)\n",
    "    start_time = time()\n",
    "    assignments = KMeans(n_clusters=k, random_state=0, max_iter=1).fit_predict(data)\n",
    "    elapsed_time = time()-start_time\n",
    "    print(f\"Finished sklearn-kmeans on {N} points in {np.round(elapsed_time,4)} seconds\")\n",
    "    del data, real_centroids\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dfbed4-43da-4761-9bfe-e7fc19d6378a",
   "metadata": {},
   "source": [
    "You'll notice that for small cases, our numpy implementation is faster.\n",
    "However, as data grows, the scikit-learn implmentation becomes faster.\n",
    "If you open up a view of the CPU utilization, you'll see that scikit-learn algorithms can use multiple CPU cores.\n",
    "\n",
    "#### Exercise:\n",
    "- Use `top` to look at CPU utilization while running KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115ea7b5-6cbb-465e-8fb5-37871b2364db",
   "metadata": {},
   "source": [
    "### Timing cuML Implementation\n",
    "\n",
    "We've spent a lot of time on CPU code, but this workshop is meant to showcase GPU acceleration.\n",
    "One of the ways we'll explore GPU acceleration is through the RAPIDS ecosystem.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/rapidsai/rapids.ai/refs/heads/main/assets/images/RAPIDS-logo.png\" alt=\"rapids\" style=\"margin: 0 auto; width: 50%;\" />\n",
    "<br>\n",
    "\n",
    "[RAPIDS](https://rapids.ai) provides unmatched speed with familiar APIs that match the most popular PyData libraries. Built on state-of-the-art foundations like NVIDIA CUDA and Apache Arrow, it unlocks the speed of GPUs with code you already know.\n",
    "\n",
    "<html>\n",
    "<table><thead>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th><b>CPU</b></th>\n",
    "    <th bgcolor=\"#7400ff\" style=\"color:white;\"><b>RAPIDS - GPU</b></th>\n",
    "  </tr></thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>Pandas</td>\n",
    "    <td><pre>import pandas as pd<br>df = pd.read_csv(\"filepath\")</pre></td>\n",
    "    <td><pre>import cudf<br>df = cudf.read_csv(\"filepath\")</pre></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Spark</td>\n",
    "    <td><pre>spark.sql(\"\"\"<br>select<br>    order<br>    count(*) as order_count<br>from<br>    orders\"\"\")</pre></td>\n",
    "    <td><pre>spark.conf.set(\"spark.plugins\",<br>\"com.nvidia.spark.SQLPlugin\")<br>spark.sql(\"\"\"<br>select<br>    order<br>    count(*) as order_count<br>from<br>    orders\"\"\")</pre></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>scikit-learn</td>\n",
    "    <td><pre>from sklearn.ensemble import RandomForestClassifier<br>clf = RandomForestClassifier()<br>clf.fit(x,y)</pre></td>\n",
    "    <td><pre>from cuml.ensemble import RandomForestClassifier<br>clf = RandomForestClassifier()<br>clf.fit(x,y)</pre></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>NetworkX</td>\n",
    "    <td><pre>import networkx as nx<br>page_rank = nx.pagerank(graph)</pre></td>\n",
    "    <td><pre>import cugraph<br>page_rank = cugraph.pagerank(graph)</pre></td>\n",
    "  </tr>\n",
    "</tbody></table>\n",
    "</html>\n",
    "\n",
    "Based on convention, there should be a [cuml.cluster.KMeans](https://docs.rapids.ai/api/cuml/stable/api/#k-means-clustering) that matches the `sklearn.cluster.KMeans` we previously used.\n",
    "\n",
    "> The documentation also states that this function can be imported with either `cuml.KMeans` and `cuml.cluster.KMeans`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df39a9c4-1e71-4fde-9e81-ca1309d29d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.cluster import KMeans\n",
    "\n",
    "k, d = 4, 10\n",
    "\n",
    "for N in [10**power for power in range(1,8)]:\n",
    "    data, real_centroids = generate_data(k=k, n=N, d=d, cmin=0, cmax=100)\n",
    "    start_time = time()\n",
    "    assignments = KMeans(n_clusters=k, random_state=0, max_iter=1).fit_predict(data)\n",
    "    elapsed_time = time()-start_time\n",
    "    print(f\"Finished sklearn-kmeans on {N} points in {np.round(elapsed_time,4)} seconds\")\n",
    "    del data, real_centroids\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99a1757-d8e5-480d-b8d3-3e16408f5123",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "While GPU performance is faster than multi-core CPU, it could be faster.\n",
    "By default, numpy creates arrays in FP64 precision.\n",
    "This is great for high numerical precision, but unless you're on a \\[VAHB\\]100 GPU with FP64 cores, performance will be limited.\n",
    "\n",
    "As shown in the table below ([source](https://www.exxactcorp.com/blog/components/NVIDIA-L40S-GPU-Compared-to-A100-and-H100-Tensor-Core-GPU)), only high end data center cards have FP64 cores.\n",
    "If you do process FP64 data, the program won't error out, but performance will be sub-optimal because RAPIDS is using emulated FP64 matrix operations on the backend.\n",
    "\n",
    "|                      | A100 80GB SXM | NVIDIA L40S  | H100 80 GB SXM |\n",
    "|----------------------|---------------|--------------|----------------|\n",
    "| GPU Architecture     | NVIDIA Ampere | Ada Lovelace | Hopper         |\n",
    "| GPU Memory           | 80GB HBM2e    | 48GB GDDR6   | 80GB HBM3      |\n",
    "| GPU Memory Bandwidth | 2039 GB/s     | 864 GB/s     | 3352 GB/s      |\n",
    "| L2 Cache             | 40MB          | 96MB         | 50MB           |\n",
    "| FP64                 | 9.7 TFLOPS    | N/A          | 33.5 TFLOPS    |\n",
    "| FP32                 | 19.5 TFLOPS   | 91.6 TFLOPS  | 66.9 TFLOPS    |\n",
    "\n",
    "The easiest way to change this is by casting the data to FP32 or lower.\n",
    "You've probably done this in PyTorch with BF16 or \"mixed-precision\" settings, but this is what's happening on the back end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fef1c9-5056-49da-9464-d3f7e18ccea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.rapids.ai/api/cuml/stable/api/#k-means-clustering\n",
    "from cuml.cluster import KMeans\n",
    "\n",
    "k, d = 4, 4\n",
    "\n",
    "for N in [10**power for power in range(1,8)]:\n",
    "    data, real_centroids = generate_data(k=k, n=N, d=d, cmin=0, cmax=100)\n",
    "    \n",
    "    # Cast the data to FP32\n",
    "    data = data.astype(np.float32)\n",
    "    \n",
    "    start_time = time()\n",
    "    assignments = KMeans(n_clusters=k, random_state=0, max_iter=1, n_init='auto').fit_predict(data)\n",
    "    elapsed_time = time()-start_time\n",
    "    print(f\"Finished cuml-kmeans on {N} points in {np.round(elapsed_time,4)} seconds\")\n",
    "    del data, real_centroids\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6072e85f-71fa-4be4-b902-1d52dbce0bad",
   "metadata": {},
   "source": [
    "In my tests on a T4, I'm seeing a 2x speedup - just from recasting the data!\n",
    "\n",
    "### Comparing the runtime of each method\n",
    "\n",
    "While we've been printing out the runtime for each implementation at different data sizes, it's hard to compare the methods.\n",
    "We can collect all the timing data in python lists and then plot them using [Matplotlib](https://matplotlib.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79bf765-a6d6-4acf-bb0d-6daa7f55ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.rapids.ai/api/cuml/stable/api/#k-means-clustering\n",
    "from cuml.cluster import KMeans as cuml_KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "from kmeans import k_means\n",
    "\n",
    "k, d = 4, 4\n",
    "\n",
    "numpy_t = [[], []]\n",
    "sklearn_t = [[], []]\n",
    "cuml_t = [[], []]\n",
    "\n",
    "for N in [10**power for power in range(1,8)]:\n",
    "    # Use same data for all implementations\n",
    "    data, real_centroids = generate_data(k=k, n=N, d=d, cmin=0, cmax=100)\n",
    "\n",
    "    start = time()\n",
    "    k_means(data, k, max_iterations=1)\n",
    "    numpy_t[1].append(time()-start)\n",
    "    numpy_t[0].append(N)\n",
    "\n",
    "    start = time()\n",
    "    assignments = KMeans(n_clusters=k, random_state=0, max_iter=1, n_init='auto').fit_predict(data)\n",
    "    sklearn_t[1].append(time()-start)\n",
    "    sklearn_t[0].append(N)\n",
    "    \n",
    "    # Cast the data to FP32 for cuML\n",
    "    data = data.astype(np.float32)\n",
    "    \n",
    "    start = time()\n",
    "    assignments = cuml_KMeans(n_clusters=k, random_state=0, max_iter=1, n_init='auto').fit_predict(data)\n",
    "    cuml_t[1].append(time()-start)\n",
    "    cuml_t[0].append(N)\n",
    "    del data, real_centroids\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72000c68-68f5-4c08-85ea-8a63a457ecd4",
   "metadata": {},
   "source": [
    "Now that we've collected the data, we can plot it.\n",
    "Since the runtimes are short and data exponentially increases, we're using a [loglog](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.loglog.html) plot to plot the data on a log scale, so trends are linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5593b9ff-9753-446a-aec3-b720429251ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.loglog(numpy_t[0], numpy_t[1], label=\"numpy\")\n",
    "plt.loglog(sklearn_t[0], sklearn_t[1], label=\"sklearn\")\n",
    "plt.loglog(cuml_t[0], cuml_t[1], label=\"cuml\")\n",
    "plt.xlabel(\"# Points\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.title(\"Time to run a single iteration of k-means\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdc37b9-4878-401b-ae6d-4871485dcda6",
   "metadata": {},
   "source": [
    "## K-means Exercises:\n",
    "\n",
    "### 1. Try increasing the data dimension or count\n",
    "- Do any methods eventually fail?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc935d7-7148-4263-b5bf-d94fc421b028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05f215d8-82eb-461b-bae2-f7073b9cc5c1",
   "metadata": {},
   "source": [
    "### 2. Try modifying `generate_data` to create [cupy arrays](https://docs.cupy.dev/en/stable/user_guide/basic.html)\n",
    "  - Does this make the cuML method faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f476b0-2b1b-4691-ac2f-2cd448e6c7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91b784fe-7581-468f-8170-448d1ac612f8",
   "metadata": {},
   "source": [
    "### 3. Try different levels of [precision](https://docs.cupy.dev/en/stable/overview.html#overview)\n",
    "  - Is there a speed up on the CPU side as well?\n",
    "  - Do the computed centroids change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe069b19-80c6-4427-a2f5-9f7d2432290f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3bec19d-93bb-45aa-a4fc-7d8254879508",
   "metadata": {},
   "source": [
    "## cuml.accel: Zero Code Change\n",
    "\n",
    "Starting in RAPIDS 25.02.01, cuML can now automatically accelerate code that uses Scikit-Learn, UMAP-Learn, and HDBSCAN.\n",
    "Simply change your invocation to include `-m cuml.accel` and the cuML libraries will be imported instead to run on NVIDIA GPUs and falling back to CPU when necessary.\n",
    "Give it a try with our kmeans code using [sklearn_kmeans.py](sklearn_kmeans.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624b7e3c-0f60-449c-8d8b-276ce36d867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export OMP_NUM_THREADS=16; python sklearn_kmeans.py\n",
    "\n",
    "# scikit-learn scripts can also be converted directly\n",
    "!python -m cuml.accel sklearn_kmeans.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d6724a-df15-4ed4-97a3-4042005d54ea",
   "metadata": {},
   "source": [
    "## CuPy: Accelerating NumPy\n",
    "\n",
    "Similar to RAPIDS, [CuPy](https://cupy.dev/) provides a NumPy-compatible interface for GPU-accelerated computing.\n",
    "CuPy leverages CUDA for parallel processing, making it ideal for applications requiring large-scale data processing and complex mathematical computations.\n",
    "\n",
    "Going full-circle, go back and open our [kmeans.py](kmeans.py) file again.\n",
    "You'll notice that the timing code we've been running is in the `__main__` portion of the script, so it will run with a normal `python kmeans.py` invocation.\n",
    "We're also only using numpy and built-in python modules.\n",
    "Since CuPy is meant to replace NumPy, we should be able to just change the import, and see what works...\n",
    "\n",
    "Make that change and try running the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d11338-39ec-4ce7-807c-b62e85102443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After changing the import\n",
    "!python kmeans.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c87d4d3-40d5-413a-a3f9-3b3fd581861f",
   "metadata": {},
   "source": [
    "If all went well, you should see a good speedup!\n",
    "\n",
    "### Exercise:\n",
    "\n",
    "- Look through code of your own to see where CuPy would be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0212e3c-549a-4247-ba41-823cd914f452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
