{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd6d7e8e-778e-42bb-97d0-d0f559cbf291",
   "metadata": {},
   "source": [
    "# Advanced Topics\n",
    "\n",
    "## Working with data that doesn't fit into GPU memory\n",
    "<hr>\n",
    "\n",
    "You've probably worked with data that is too large to fit into GPU or even system memory in the past.\n",
    "In this section, we're going to explore methods to work with data that is larger than GPU memory.\n",
    "\n",
    "### Quickly generating data\n",
    "\n",
    "Since we're going to be generating larger datasets, we're also going to use a `mp_generate_data` function from [extras.py](extras.py).\n",
    "If you look at the code, you'll notice that it not only is multi-core, but also generates the data in FP32.\n",
    "\n",
    "Run the code below to test out the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe81b694-3730-4e1e-8bfc-7d45232d9f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extras import generate_data, mp_generate_data\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "k, d = 4, 4\n",
    "\n",
    "for N in [10**power for power in range(6,9)]:\n",
    "    # Testing the origninal generate_data\n",
    "    print(f\"Generating {N} points\")\n",
    "    start_time = time()\n",
    "    data, real_centroids = generate_data(k=k, n=N, d=d, cmin=0, cmax=100)\n",
    "    elapsed_orig = time()-start_time\n",
    "    data_mb = np.round(data.size * data.itemsize / 2**20, 3)\n",
    "    print(f\" - original  (FP64) - {elapsed_orig:5.2f} sec - {data_mb:4.2f} MB\")\n",
    "\n",
    "    # Testing mp_generate_data\n",
    "    start_time = time()\n",
    "    data, real_centroids = mp_generate_data(k=k, n=N, d=d, cmin=0, cmax=100)\n",
    "    elapsed_orig = time()-start_time\n",
    "    data_mb = np.round(data.size * data.itemsize / 2**20, 3)\n",
    "    print(f\" - multiproc (FP32) - {elapsed_orig:5.2f} sec - {data_mb:4.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d836c62e-075f-4a38-ba4e-8b73c159f219",
   "metadata": {},
   "source": [
    "Looking at the results, `mp_generate_data` is worth using when generating 10,000,000 or more data points.\n",
    "The FP32 values it generates are also not only more suited to running on the GPU, but they take up less memory as well (at the expense of precision).\n",
    "\n",
    "### Testing the limits of GPU memory\n",
    "\n",
    "Now that we can generate data quickly, we can start exploring the limits of GPU-accelerated algorithms.\n",
    "\n",
    "> This was developed on a L40S with 48GB of VRAM. Increase the data size if 800M points does not fail on your GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd7d21f-3afa-4e48-8215-3bad02d61295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary for SKL code to run on systems with > 64 cores\n",
    "#import os\n",
    "#os.environ['OMP_NUM_THREADS']='32'\n",
    "\n",
    "from sklearn.cluster import KMeans as skl_KMeans\n",
    "from cuml.cluster import KMeans as cuml_KMeans\n",
    "from extras import generate_data, mp_generate_data\n",
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "k, d = 4, 4\n",
    "\n",
    "for N in [10**8, 8*10**8]:# for power in range(7,10)]:\n",
    "    data, real_centroids = mp_generate_data(k=k, n=N, d=d, cmin=0, cmax=100)\n",
    "    data_mb = np.round(data.size * data.itemsize / 2**20, 3)\n",
    "    print(f\"Generated {N} (FP32) points ({data_mb} MB)\")\n",
    "\n",
    "    # Takes about 4 minutes on 32 cores\n",
    "    #start_time = time()\n",
    "    #assignments = skl_KMeans(n_clusters=k, random_state=0, max_iter=1).fit_predict(data)\n",
    "    #elapsed_time = time()-start_time\n",
    "    #print(f\"  - sklearn {np.round(elapsed_time,4)} seconds\")\n",
    "    \n",
    "    start_time = time()\n",
    "    assignments = cuml_KMeans(n_clusters=k, random_state=0, n_init='auto', max_iter=1).fit_predict(data)\n",
    "    elapsed_time = time()-start_time\n",
    "    print(f\"  - cuml {np.round(elapsed_time,4)} seconds\")\n",
    "    del data, real_centroids, assignments\n",
    "print(\"Done\") # Tell me when to stop waiting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c52acd-bbc8-4961-93ba-95541aad8dad",
   "metadata": {},
   "source": [
    "You can see that GPU-based KMeans failed on the second data set because it ran out of memory.\n",
    "While slower, the CPU-based scikit-learn KMeans version completed because it ran from (the usually larger) system memory (RAM).\n",
    "\n",
    "### RAPIDS Memory Manager\n",
    "\n",
    "While it may see like the end of the road, the [RAPIDS Memory Manager](https://docs.rapids.ai/api/rmm/stable/) (RMM) was created help allocate GPU memory in highly configurable ways.\n",
    "One of those ways is to use Unified Virtual Memory (UVM) provides a single, unified memory address space accessible from both the CPU and GPU, simplifying memory management and enabling efficient data sharing between processors.\n",
    "UVM also allows data in system (CPU) RAM to be larger than GPU memory and handles all the transactions necessary when computing on it.\n",
    "\n",
    "UVM can be enabled by setting `managed_memory=True` when initializing RMM.\n",
    "Do this and try re-running cuML KMeans that previously failed due to OOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d6001c-6078-4623-aa28-effc63c9b8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rmm\n",
    "# Enabled managed memory (allows spilling)\n",
    "rmm.reinitialize(managed_memory=True)\n",
    "from cuml.cluster import KMeans as cuml_KMeans\n",
    "from extras import mp_generate_data\n",
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "k, d = 4, 4\n",
    "\n",
    "for N in [10**8, 8*10**8]:# for power in range(7,10)]:\n",
    "    data, real_centroids = mp_generate_data(k=k, n=N, d=d, cmin=0, cmax=100)\n",
    "    data_mb = np.round(data.size * data.itemsize / 2**20, 3)\n",
    "    print(f\"Generated {N} (FP32) points ({data_mb} MB)\")\n",
    "\n",
    "    # Skip CPU since it works\n",
    "    \n",
    "    start_time = time()\n",
    "    assignments = cuml_KMeans(n_clusters=k, random_state=0, n_init='auto', max_iter=1).fit_predict(data)\n",
    "    elapsed_time = time()-start_time\n",
    "    print(f\"  - cuml {np.round(elapsed_time,4)} seconds\")\n",
    "    del data, real_centroids, assignments\n",
    "print(\"Done\") # I sometimes wait for another step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681c321f-db91-498d-89e0-548bccefd74d",
   "metadata": {},
   "source": [
    "You can see that there is a performance hit, but it does complete without errors and no longer limits your computation to data that fits in GPU memory.\n",
    "\n",
    "### Exercises:\n",
    "\n",
    "- Is using a GPU with UVM faster than the CPU version of K-means for large data sets?\n",
    "- Try larger datasets to see if there's a failure point\n",
    "  - Make sure to look at system memory (RAM)\n",
    "\n",
    "## Using Multiple GPUs\n",
    "<hr>\n",
    "\n",
    "For multiple GPUs, we recommend [Dask](https://docs.dask.org/en/stable/) and [dask-cuda](https://docs.rapids.ai/api/dask-cuda/nightly/quickstart/). Dask is an open-source Python library that enables parallel and distributed computing for large-scale data processing, providing advanced data structures and algorithms that scale from single machines to large clusters.\n",
    "dask-cuda takes this a step further by doing this with GPU data structures like CuPy arrays and cuDF dataframes for scaling computations to multiple GPUs and GPU-nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49414f9-42d2-49fb-ae14-a13627f24015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import dask\n",
    "# Specify that the dask cluster will use cupy (gpu-numpy) for arrays\n",
    "dask.config.set({\"array.backend\": \"cupy\", \"logging.distributed\": \"error\"})\n",
    "\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "import dask.array as da\n",
    "# Import dask version of KMeans\n",
    "from cuml.dask.cluster import KMeans as cuml_KMeans\n",
    "\n",
    "# Create a local (single-node) cluster and used RMM managed memory (OOM otherwise)\n",
    "cluster = LocalCUDACluster(rmm_managed_memory=True, rmm_pool_size=0.9)\n",
    "# Connect to the cluster\n",
    "client = Client(cluster)\n",
    "\n",
    "# Generate data as dask arrays\n",
    "def dask_generate_data(k=3, n=10, d=2, cmin=0, cmax=10):\n",
    "    # Make sure to specify float32!\n",
    "    C = da.random.random((k,d), dtype=cp.float32)*(cmax-cmin)-cmin\n",
    "    R = da.random.normal(loc=0, scale=1, size=(n, d), dtype=cp.float32)+C[da.random.choice(k, n, replace=True)]\n",
    "    return R, C\n",
    "\n",
    "k, d = 4, 4\n",
    "\n",
    "for N in [10**8, 8*10**8]:# for power in range(7,10)]:\n",
    "    dask_data, real_centroids = dask_generate_data(k=k, n=N, d=d, cmin=0, cmax=100)\n",
    "    data_mb = np.round(dask_data.size * dask_data.itemsize / 2**20, 3)\n",
    "    print(f\"Generated {N} points ({data_mb} MB)\")\n",
    "    \n",
    "    start_time = time()\n",
    "    assignments = cuml_KMeans(n_clusters=k, random_state=0, n_init='auto', max_iter=1).fit_predict(dask_data)\n",
    "    assignments.compute()\n",
    "    #print(assignments.map_blocks(cp.asnumpy).compute()[:10])\n",
    "    elapsed_time = time()-start_time\n",
    "    print(f\"  - dask+cuml {np.round(elapsed_time,4)} seconds\")\n",
    "    del dask_data, real_centroids, assignments\n",
    "print(\"Done\") # I sometimes wait for another step\n",
    "\n",
    "# Disconnect from Dask cluster\n",
    "client.close()\n",
    "# Shut down Dask cluster\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fe20e1-bda1-49f0-8af4-3a53c51244d5",
   "metadata": {},
   "source": [
    "You'll notice that using Dask on two L40S GPUs (no NVLink) was actually slower for a 1.5GB dataset, but faster for the 12GB scale.\n",
    "Whenever an algorithm is scaled to multiple workers, there's overhead from communication that limits the speed.\n",
    "You saw this with `mp_generate_data` and now with Dask.\n",
    "\n",
    "> Make sure to restart the kernel between runs\n",
    "\n",
    "### Exercises:\n",
    "\n",
    "- If you can, try running on more than two GPUs\n",
    "    - What does the scaling efficiency look like?\n",
    "    - Does it improve for data sets that fit in GPU memory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea99fec-9d37-4da7-a87a-ac9eb5304c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "707d903a-7087-486b-9d67-36d8ea7d3420",
   "metadata": {},
   "source": [
    "- What happens if you exclude RMM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368701b0-0f8f-423a-a2e7-a7528de28c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd5506b9-41f4-4457-8b11-6d43ef9843d1",
   "metadata": {},
   "source": [
    "- What happens if you exclude the pool size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaa9747-e60f-42bc-8588-3c12f522eeb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c127d12-ef89-4a71-b083-4c3602a039f0",
   "metadata": {},
   "source": [
    "- Try [enabling UCX communication](https://docs.rapids.ai/api/dask-cuda/stable/examples/ucx/) on a system with NVLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7b0c6d-8c3a-4c1c-b843-fbb48fd4acda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09a6fe05-30d4-4842-9617-24a6ee78c6b1",
   "metadata": {},
   "source": [
    "- What scale of data does using Dask for multiple-GPUs make sense on your GPUs/system?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5766a4d1-22a2-42f3-8816-0a920d87b71f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b7045a3-7bec-421a-b735-7bf9096359df",
   "metadata": {},
   "source": [
    "### Additional Dask features\n",
    "\n",
    "Dask is also capable of scaling to GPUs on multiple nodes.\n",
    "We're not going to cover this topic today, but take a look at their [cluster deployment documentation](https://docs.dask.org/en/stable/deploying.html#) to figure out which method is suitable for your compute infrastructure.\n",
    "\n",
    "In addition to scaling algorithms, Dask is commonly known for splitting large data objects (chunking) and persisting chunks in different tiers of memory (spilling).\n",
    "There's support for this at the GPU level (without RMM), but has best support with [cuDF data frames](https://docs.rapids.ai/api/dask-cudf/stable/). Some RAPIDS alorithms (like KMeans) also may not work well with this feature.\n",
    "\n",
    "## Topics not covered, but worth exploring\n",
    "\n",
    "- [Numba](https://numba.readthedocs.io/en/stable/cuda/index.html) - Numba is a compiler for Python array and numerical functions that gives you the power to speed up your applications with high performance functions written directly in Python.\n",
    "- [NVIDIA cuPyNumeric](https://docs.nvidia.com/cupynumeric/latest/index.html) - NVIDIA-developed distributed and accelerated drop-in replacement for NumPy built on top of the Legate framework\n",
    "  - CuPy is recommended for single GPU code and has better support for NumPy functions\n",
    "  - Meant to make multi-GPU/node scaling easier than Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b729b0-999b-4dd9-8aea-ff61ccfb551b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
